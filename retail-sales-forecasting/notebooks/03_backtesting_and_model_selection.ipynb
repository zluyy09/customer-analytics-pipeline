{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 â€” Backtesting & Model Selection\n",
    "Rolling origin evaluation with WAPE, sMAPE, MASE, Bias. Compare SARIMAX, Prophet, and Global GBM. Focus on **holiday weeks** and **tail stores**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002773 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1553\n",
      "[LightGBM] [Info] Number of data points in the train set: 56071, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 16317.107284\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 70259, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 16324.169269\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003607 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1563\n",
      "[LightGBM] [Info] Number of data points in the train set: 84466, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 16327.638236\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001915 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1568\n",
      "[LightGBM] [Info] Number of data points in the train set: 98782, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 16196.269210\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001385 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1573\n",
      "[LightGBM] [Info] Number of data points in the train set: 113178, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 16184.467703\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001429 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1578\n",
      "[LightGBM] [Info] Number of data points in the train set: 127644, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 16436.145807\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1586\n",
      "[LightGBM] [Info] Number of data points in the train set: 142125, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16660.343980\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1588\n",
      "[LightGBM] [Info] Number of data points in the train set: 156582, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16578.698964\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004489 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1588\n",
      "[LightGBM] [Info] Number of data points in the train set: 171066, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16530.879994\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1588\n",
      "[LightGBM] [Info] Number of data points in the train set: 185611, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16497.037783\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002657 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1588\n",
      "[LightGBM] [Info] Number of data points in the train set: 200096, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16487.273451\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003483 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1588\n",
      "[LightGBM] [Info] Number of data points in the train set: 214562, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 16510.779290\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>wape</th>\n",
       "      <th>smape</th>\n",
       "      <th>bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.076477</td>\n",
       "      <td>16.065903</td>\n",
       "      <td>0.020593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.077652</td>\n",
       "      <td>15.852307</td>\n",
       "      <td>-0.009437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.097663</td>\n",
       "      <td>18.227117</td>\n",
       "      <td>0.034755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.092208</td>\n",
       "      <td>17.228288</td>\n",
       "      <td>-0.054550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.184332</td>\n",
       "      <td>22.964890</td>\n",
       "      <td>-0.003418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.156549</td>\n",
       "      <td>27.307839</td>\n",
       "      <td>0.056794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.089437</td>\n",
       "      <td>23.338491</td>\n",
       "      <td>-0.025783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.081082</td>\n",
       "      <td>18.157680</td>\n",
       "      <td>-0.002226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.121061</td>\n",
       "      <td>21.472350</td>\n",
       "      <td>0.003751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.081661</td>\n",
       "      <td>18.341053</td>\n",
       "      <td>-0.037523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.078020</td>\n",
       "      <td>16.935656</td>\n",
       "      <td>-0.004206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.089674</td>\n",
       "      <td>19.053085</td>\n",
       "      <td>0.059904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fold      wape      smape      bias\n",
       "0      1  0.076477  16.065903  0.020593\n",
       "1      2  0.077652  15.852307 -0.009437\n",
       "2      3  0.097663  18.227117  0.034755\n",
       "3      4  0.092208  17.228288 -0.054550\n",
       "4      5  0.184332  22.964890 -0.003418\n",
       "5      6  0.156549  27.307839  0.056794\n",
       "6      7  0.089437  23.338491 -0.025783\n",
       "7      8  0.081082  18.157680 -0.002226\n",
       "8      9  0.121061  21.472350  0.003751\n",
       "9     10  0.081661  18.341053 -0.037523\n",
       "10    11  0.078020  16.935656 -0.004206\n",
       "11    12  0.089674  19.053085  0.059904"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "RAW = Path(\"../data/raw\")\n",
    "train = pd.read_csv(RAW/\"train.csv\")\n",
    "stores = pd.read_csv(RAW/\"stores.csv\")\n",
    "features = pd.read_csv(RAW/\"features.csv\")\n",
    "for df in [train, features]:\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "df = (train.merge(stores, on='Store', how='left')\n",
    "           .merge(features, on=['Store','Date'], how='left')\n",
    "      ).sort_values(['Store','Dept','Date'])\n",
    "df = df.drop(columns=['IsHoliday_y'])  # remove duplicate IsHoliday\n",
    "df = df.rename(columns={'IsHoliday_x': 'IsHoliday'})\n",
    "df['IsHoliday'] = df['IsHoliday'].astype(bool)\n",
    "\n",
    "# Features (minimal for demo; reuse engineered features from notebook 02 for real run)\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Week'] = df['Date'].dt.isocalendar().week.astype(int)\n",
    "for k in [1,2,3,4,13,52]:\n",
    "    df[f'lag_{k}'] = df.groupby(['Store','Dept'])['Weekly_Sales'].shift(k)\n",
    "\n",
    "y = 'Weekly_Sales'\n",
    "X_cols = ['IsHoliday','Year','Week'] + [c for c in df.columns if c.startswith('lag_')]\n",
    "\n",
    "def wape(y_true, y_pred):\n",
    "    return np.abs(y_true - y_pred).sum() / (np.abs(y_true).sum() + 1e-9)\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    return 100/len(y_true) * np.sum(2*np.abs(y_pred - y_true) / (np.abs(y_true)+np.abs(y_pred)+1e-9))\n",
    "\n",
    "def mase(y_true, y_pred, m=52):\n",
    "    # Seasonal naive denominator\n",
    "    d = np.abs(y_true[m:] - y_true[:-m]).mean()\n",
    "    return np.abs(y_true - y_pred).mean() / (d + 1e-9)\n",
    "\n",
    "# Rolling origin splits\n",
    "dates = sorted(df['Date'].unique())\n",
    "folds = []\n",
    "for split in dates[int(len(dates)*0.5): int(len(dates)*0.9): 5]:\n",
    "    train_mask = df['Date'] <= split\n",
    "    valid_mask = (df['Date'] > split) & (df['Date'] <= split + np.timedelta64(28,'D'))\n",
    "    if valid_mask.sum() < 1000: \n",
    "        continue\n",
    "    folds.append((train_mask, valid_mask))\n",
    "\n",
    "scores = []\n",
    "for i,(tr,va) in enumerate(folds,1):\n",
    "    tr_df, va_df = df[tr].copy(), df[va].copy()\n",
    "    tr_df = tr_df.dropna(subset=X_cols+[y])\n",
    "    va_df = va_df.dropna(subset=X_cols+[y])\n",
    "    model = LGBMRegressor(n_estimators=1000, learning_rate=0.03, num_leaves=64, random_state=42)\n",
    "    model.fit(tr_df[X_cols], tr_df[y])\n",
    "    pred = model.predict(va_df[X_cols])\n",
    "    scores.append({\n",
    "        'fold': i,\n",
    "        'wape': wape(va_df[y].values, pred),\n",
    "        'smape': smape(va_df[y].values, pred),\n",
    "        'bias': (pred.sum()-va_df[y].sum())/(va_df[y].sum()+1e-9)\n",
    "    })\n",
    "\n",
    "pd.DataFrame(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b9295c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
